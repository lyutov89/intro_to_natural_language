{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
    "Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'\n",
    "\n",
    "2) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function).\n",
    "\n",
    "3) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "4) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "5) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "7) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "8) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "9) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "10) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "11) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "12) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "13) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "14) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "15) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‑)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      "id       49159 non-null int64\n",
      "label    31962 non-null float64\n",
      "tweet    49159 non-null object\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combine_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Изменим регистр твитов на нижний с помощью .lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: HTMLParser in c:\\users\\anatoly\\anaconda3\\lib\\site-packages (0.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import HTMLParser\n",
    "#parser = HTMLParser()\n",
    "#combine_df['tweet'] = combine_df['tweet'].apply(lambda x: parser.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_text(text):\n",
    "    text = re.sub(r'@[\\w]*',' ', text)\n",
    "    text = text.lower()\n",
    "    new_text = ''\n",
    "    for word in text.split():\n",
    "        if word in apostrophe_dict.keys():\n",
    "            word = apostrophe_dict[word]\n",
    "        if word in short_word_dict.keys():\n",
    "            word = short_word_dict[word]\n",
    "        if word in emoticon_dict.keys():\n",
    "            word = emoticon_dict[word]            \n",
    "        new_text = new_text + word + ' ' \n",
    "    new_text = re.sub(r'[^\\w\\s]',' ', new_text)\n",
    "    new_text = re.sub(r'[^a-zA-Z]',' ', new_text)\n",
    "\n",
    "    new_text = ' '.join([w for w in new_text.split() if len(w)>1])\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_text_vect = np.vectorize(change_text)\n",
    "combine_df['tweet'] = combine_df['tweet'].apply(lambda x: change_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        when father is dysfunctional and is so selfish...\n",
       "1        thanks for lyft credit cannot use cause they d...\n",
       "2                                      bihday your majesty\n",
       "3          model love you take with you all the time in ur\n",
       "4                        factsguide society now motivation\n",
       "                               ...                        \n",
       "49154    thought factory left right polarisation trump ...\n",
       "49155    feeling like mermaid hairflip neverready forma...\n",
       "49156    hillary campaigned today in ohio omg amp used ...\n",
       "49157    happy at work conference right mindset leads t...\n",
       "49158    my song so glad free download shoegaze newmusi...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [when, father, is, dysfunctional, and, is, so,...\n",
       "1    [thanks, for, lyft, credit, can, not, use, cau...\n",
       "Name: tweet_token, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "combine_df['tweet_token'] = combine_df['tweet'].apply(lambda x: nltk.tokenize.word_tokenize(x))\n",
    "combine_df['tweet_token'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [when, father, is, dysfunctional, and, is, so,...\n",
       "1    [thanks, for, lyft, credit, cannot, use, cause...\n",
       "Name: tweet_token, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet_token'] = combine_df['tweet'].apply(lambda x: x.split())\n",
    "combine_df['tweet_token'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"weren't\", 'yourselves', 'or', 'wouldn', \"don't\", \"isn't\", 'll', 'their', 'mightn', 'needn', 'my', 'most', 'has', 'doesn', 'd', 'other', 'until', 'was', 'off', 'couldn', \"hasn't\", 'can', \"you'll\", 'from', 'over', 'own', 'be', 'don', \"hadn't\", 'weren', 'a', 'to', 'our', 'but', 'below', 'himself', 'does', \"should've\", 'at', 'shouldn', 'wasn', 'of', 'is', 'few', 'ma', 'we', 'against', 'in', 'only', 'some', 'where', 'each', 'will', \"shan't\", \"mustn't\", 'for', 'they', 're', \"needn't\", \"doesn't\", \"she's\", 'there', 'its', 'while', 'again', 'if', 'whom', 'here', 'itself', 'having', 'were', 'how', 'all', 'any', \"you'd\", 'hers', 'hasn', 'aren', 'as', 'have', 'out', 'm', 'and', 'theirs', 'that', 'because', 'shan', 'same', 'further', 'on', \"aren't\", 'with', 'ain', 'now', \"that'll\", 'hadn', 's', 'those', \"it's\", 'very', 'about', 'once', 'had', 'during', 'being', \"couldn't\", 've', 'been', 'isn', 'by', 'me', 'down', 'both', 'through', 'such', 'them', 'the', 'between', 'nor', 'haven', 'into', \"didn't\", 'this', 'when', 'yourself', 'too', 'herself', \"wouldn't\", 'o', 'ourselves', 'myself', 'above', 'under', 'not', 'didn', 'yours', 'am', 'an', 'after', \"you're\", 'themselves', 'did', \"haven't\", 'mustn', 'why', 'up', 'doing', 't', \"wasn't\", 'him', 'ours', 'are', 'more', 'do', \"mightn't\", 'which', \"won't\", 'than', 'before', 'won', 'you', 'i', \"you've\", 'her', 'his', 'so', 'what', 'it', 'y', 'she', 'then', 'just', 'your', 'he', 'should', \"shouldn't\", 'these', 'no', 'who'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Football', 'greatest', 'play', 'world']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Football is the greatest play in the world\"\n",
    "\n",
    "words = sentence.split()\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_without_stop_words(words, stop_words):\n",
    "    words = list(words)\n",
    "    without_stop_words = [word for word in words if not word in stop_words]\n",
    "    return without_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [father, dysfunctional, selfish, drags, kids, ...\n",
       "1        [thanks, lyft, credit, cannot, use, cause, off...\n",
       "2                                        [bihday, majesty]\n",
       "3                            [model, love, take, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "49154    [thought, factory, left, right, polarisation, ...\n",
       "49155    [feeling, like, mermaid, hairflip, neverready,...\n",
       "49156    [hillary, campaigned, today, ohio, omg, amp, u...\n",
       "49157    [happy, work, conference, right, mindset, lead...\n",
       "49158    [song, glad, free, download, shoegaze, newmusi...\n",
       "Name: tweet_token_filtered, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(lambda x: get_without_stop_words(x, stop_words))\n",
    "combine_df['tweet_token_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anatoly\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\anatoly\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_stemmer(stemmer, words):\n",
    "    \"\"\"\n",
    "    Print the results of stemmind using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for word in words:\n",
    "        res.append(stemmer.stem(word))\n",
    "    return res      \n",
    "    \n",
    "def get_lemmatizer(lemmatizer, words, pos):\n",
    "    \"\"\"\n",
    "    Print the results of lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for word in words:\n",
    "        res.append(lemmatizer.lemmatize(word, pos))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lem(tweet, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "combine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(lambda x: get_stemmer(stemmer, x))\n",
    "combine_df['tweet_lemmatized'] = combine_df['tweet_token_filtered'].apply(lambda x: get_lem(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [father, dysfunct, selfish, drag, kid, dysfunc...\n",
       "1        [thank, lyft, credit, cannot, use, caus, offer...\n",
       "2                                        [bihday, majesti]\n",
       "3                            [model, love, take, time, ur]\n",
       "4                              [factsguid, societi, motiv]\n",
       "                               ...                        \n",
       "49154    [thought, factori, left, right, polaris, trump...\n",
       "49155    [feel, like, mermaid, hairflip, neverreadi, fo...\n",
       "49156    [hillari, campaign, today, ohio, omg, amp, use...\n",
       "49157    [happi, work, confer, right, mindset, lead, cu...\n",
       "49158    [song, glad, free, download, shoegaz, newmus, ...\n",
       "Name: tweet_stemmed, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet_stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [father, dysfunctional, selfish, drag, kid, dy...\n",
       "1        [thanks, lyft, credit, cannot, use, cause, off...\n",
       "2                                        [bihday, majesty]\n",
       "3                            [model, love, take, time, ur]\n",
       "4                        [factsguide, society, motivation]\n",
       "                               ...                        \n",
       "49154    [thought, factory, left, right, polarisation, ...\n",
       "49155    [feel, like, mermaid, hairflip, neverready, fo...\n",
       "49156    [hillary, campaign, today, ohio, omg, amp, use...\n",
       "49157    [happy, work, conference, right, mindset, lead...\n",
       "49158    [song, glad, free, download, shoegaze, newmusi...\n",
       "Name: tweet_lemmatized, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet_lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "result = combine_df\n",
    "upd = open('combine_df.pkl','ab')\n",
    "pickle.dump(result,upd)\n",
    "upd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тема “Создание признакового пространства”\n",
    "\n",
    "Продолжим обработку данных с Твиттера. \n",
    "\n",
    "1. Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "Исключим стоп-слова с помощью stop_words='english'. \n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().\n",
    " \n",
    "2. Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().\n",
    "\n",
    "3. Создайте мешок слов с помощью sklearn.feature_extraction.text.HashingVectorizer.fit_transform().\n",
    "Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "Ограничим количество фичей, с помощью n_features = 1000.(можно изменить)\n",
    "Исключим стоп-слова с помощью stop_words='english'.\n",
    "\n",
    "4. Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "X_train_CV_stem = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CV_stem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CV_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_lemmatized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "X_train_CV_lem = pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accept  account  act  action  actor  actually  adapt  \\\n",
       "0     0           0       0        0    0       0      0         0      0   \n",
       "1     0           0       0        0    0       0      0         0      0   \n",
       "\n",
       "   add  ...  yes  yesterday  yo  yoga  york  young  youth  youtube  yr  yummy  \n",
       "0    0  ...    0          0   0     0     0      0      0        0   0      0  \n",
       "1    0  ...    0          0   0     0     0      0      0        0   0      0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CV_lem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_CV_lem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english')\n",
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "X_train_tfidf_stem = pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "   yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n",
       "0   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "1   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_stem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_lemmatized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "X_train_tfidf_lem = pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  accept  account  act  action  actor  actually  adapt  \\\n",
       "0   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "1   0.0         0.0     0.0      0.0  0.0     0.0    0.0       0.0    0.0   \n",
       "\n",
       "   add  ...  yes  yesterday   yo  yoga  york  young  youth  youtube   yr  \\\n",
       "0  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0    0.0      0.0  0.0   \n",
       "1  0.0  ...  0.0        0.0  0.0   0.0   0.0    0.0    0.0      0.0  0.0   \n",
       "\n",
       "   yummy  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_lem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf_lem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_stemmed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vectorizer = text.HashingVectorizer(n_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_stemmed_HV = hash_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hv_stem = pd.DataFrame(bow_stemmed_HV.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  990  991  992  993  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hv_stem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hv_stem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = combine_df['tweet_lemmatized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_hash_lemmatized = hash_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hv_lem = pd.DataFrame(bow_hash_lemmatized.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  990  991  992  993  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hv_lem.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 1000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hv_lem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tbd as soon as I finish it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Stuning even for the non-gamer: This sound tra...      2\n",
       "1  The best soundtrack ever to anything.: I'm rea...      2\n",
       "2  Amazing!: This soundtrack is my favorite music...      2\n",
       "3  Excellent Soundtrack: I truly like this soundt...      2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...      2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "corpus in DF\n",
    "\"\"\"\n",
    "\n",
    "corpus = open('corpus').read()\n",
    "\n",
    "labels, texts = [], []\n",
    "\n",
    "for i, line in enumerate(corpus.split('\\n')):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(' '.join(content[1:]))\n",
    "    \n",
    "corpusDF = pd.DataFrame()\n",
    "corpusDF['text'] = texts\n",
    "corpusDF['label'] = labels\n",
    "corpusDF['label'] = corpusDF['label'].map({'__label__2': 2, '__label__1': 1})\n",
    "corpusDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corpus, y_corpus = corpusDF['text'], corpusDF['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% - в тест, 70% - в трейн. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stemm = combine_df['tweet_stemmed'].apply(lambda x: \" \".join(x))\n",
    "doc_lemm = combine_df['tweet_lemmatized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stem = doc_stemm[:subj]\n",
    "X_train_lem = doc_lemm[:subj]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Anatoly/Desktop/GU/Python-general/intro_to_natural_language_/lesson_1/combine_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, cannot, use, cause...</td>\n",
       "      <td>[thanks, lyft, credit, cannot, use, cause, off...</td>\n",
       "      <td>[thank, lyft, credit, cannot, use, caus, offer...</td>\n",
       "      <td>[thanks, lyft, credit, cannot, use, cause, off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0    model love you take with you all the time in ur   \n",
       "4   5    0.0                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, cannot, use, cause...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, you, take, with, you, all, the, ...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, cannot, use, cause, off...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, cannot, use, caus, offer...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, cannot, use, cause, off...  \n",
       "2                                  [bihday, majesty]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                  [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_buf = pd.read_pickle(path)\n",
    "df_buf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    29720\n",
       "1.0     2242\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_buf['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = combine_df['label'][:subj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    27913\n",
       "1.0     2087\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n",
    "#Вообще-то говоря, налицо дисбаланс классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['method']).set_index('method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standart</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>CV stem</td>\n",
       "      <td>0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CV lem</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tfidf stem</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tfidf lem</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HV stem</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HV lem</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            standart\n",
       "method              \n",
       "CV stem       0.0102\n",
       "CV lem        0.0069\n",
       "tfidf stem    0.0024\n",
       "tfidf lem     0.0013\n",
       "HV stem       0.0012\n",
       "HV lem        0.0008"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key_vac, val_vec in {\n",
    "    'CV': text.CountVectorizer(max_df=0.9, max_features=1000, stop_words='english'), \n",
    "    'tfidf': text.TfidfVectorizer(max_df=0.9, max_features=1000, stop_words='english'), \n",
    "    'HV': text.HashingVectorizer(n_features=1000, stop_words='english')\n",
    "}.items():\n",
    "    \n",
    "    for key_tweet, val_tweet in {\n",
    "        'stem': X_train_stem, \n",
    "        'lem': X_train_lem\n",
    "    }.items():\n",
    "        \n",
    "        X_train = val_vec.fit_transform(val_tweet)\n",
    "        X_test = val_vec.transform(X_corpus)\n",
    "        \n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        \n",
    "        results.loc[f'{key_vac} {key_tweet}', 'standart'] = accuracy_score(y_corpus, y_test_pred)\n",
    "\n",
    "results.plot(kind='barh', grid=True, title='Accuracy score', figsize=(8, 8))\n",
    "plt.show()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Низкий скор можно объяснить дисбалансом классов. CV - Лучший скор на стем. токенах. HV - худший скор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAHiCAYAAAC9RyU7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de7iVdZ3//+ebgyBiah6aFAs8IScFFWskDdLJiu9gU6Y4+g01R9PGQavJw0yFzm/SrhwzR6fyq4nTqEw2mjkdNd2aMyZKIohK6ECKNKmUBCrI4f37Yy1ogRvYfFiLtfbez8d17Yt1nz7rfb8D96vPfd9rRWYiSZJUokezC5AkSZ2XQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQurmIaIuI30dEn2bXIqnzMUhI3VhEDASOBBKYsI3fu9e2fL9G6SrnIZUySEjd28eBXwBTgUm1GyJi+4j4p4j4dUQsiYgHI2L76rb3RMR/R8QrEfF8RJxaXd8WEWfUjHFqRDxYs5wR8amImAfMq677WnWMP0TEjIg4smb/nhFxcUQ8GxFLq9v3johrI+KfNqj3rog4b8MTjIqvRsSL1fOYFRHDO3COEyJiTvUc2yJiSM2YCyLigoiYBbwaEb0iYs+I+I+IeCki5kfE3xT+byJ1KgYJqXv7OHBz9efYiHhbzbYrgEOBI4C3Ap8D1kTEO4AfAf8M7A6MBGZuwXt+GHgXMLS6/Eh1jLcCtwC3RUTf6rZPAycBHwLeApwOvAbcBJwUET0AImI34Gjg1nbe7/3AUcABwM7AicDizZzjAdWxzque4w+BuyJiu5pxTwLGV8dcA9wFPA7sVa3lvIg4dgv6InVKBgmpm4qI9wDvBL6TmTOAZ4G/rG7rQeWX9uTMfCEzV2fmf2fmCuBk4J7MvDUzV2bm4szckiBxWWb+LjNfB8jMf6uOsSoz/wnoAwyu7nsG8PeZOTcrHq/uOx1YQuUXNsBEoC0zf9vO+60EdgQOBCIzn8rM32zmHE8EfpCZd2fmSiqBY3sqgWOtqzPz+ep5jAZ2z8xLM/ONzPwf4P9V65K6NIOE1H1NAn6amS9Xl2/hj5c3dgP6UgkXG9p7I+s76vnahYj4TEQ8Vb208AqwU/X9N/deNwGnVF+fAny7vZ0y817gGuBa4LcRcV1EvIVNn+OewK9rxlhTrXuvjZzHO4E9q5dBXqmex8VA7QyP1CUZJKRuqHofwAnAeyPifyPif4HzgYMj4mDgZWA5sG87hz+/kfUArwL9apb/pJ191n3lcPV+iAuqteySmTtTmWmIDrzXvwHHVesdAnxvI/uRmVdn5qHAMCqXOP6WTZ/jIirhYG2dQSXUvNDeeVTrnJ+ZO9f87JiZH9pYTVJXYZCQuqcPA6up3KcwsvozBPg58PHq/wP/FnBl9SbCnhHxp9VHRG8GjomIE6o3Ge4aESOr484EPhIR/SJiP+ATm6ljR2AV8BLQKyK+QOVeiLWuB/4hIvav3jR5UETsCpCZC6ncX/Ft4D/WXirZUESMjoh3RURvKkFnObB6M+f4HWB8RBxdPe4zwArgvzdyHtOBP1RvwNy+OtbwiBi9mfOXOj2DhNQ9TQJuzMznMvN/1/5QuQRwcvWRxs8Cs6n8sv4d8GWgR2Y+R+Xmx89U188EDq6O+1XgDeC3VC493LyZOn5C5cbNX1G5lLCc9S8ZXEnll/pPgT8AN1C5V2Gtm4ARbOSyRtVbqNyv8Pvqeyymcs8DmzjHuVQul/wzlZmLPwf+PDPfaO8NMnN1dZ+RwPzqMddTuUwjdWmRmZvfS5JaUEQcReUSx8DqDIOkbcwZCUmdUvWSw2TgekOE1DwGCUmdTvXDoV4B3g5c1eRypG7NSxuSJKmYMxKSJKmYQUKSJBXzW+sK7Lzzzrnffvs1u4wu4dVXX2WHHXZodhldgr2sL/tZP/ayfprVyxkzZrycmbu3t80gUeBtb3sbjz76aLPL6BLa2toYO3Zss8voEuxlfdnP+rGX9dOsXkbErze2zUsbkiSpmEFCkiQVM0hIkqRi3iMhSWo5K1euZOHChSxfvrzZpbSUnXbaiaeeeqph4/ft25cBAwbQu3fvDh9jkJAktZyFCxey4447MnDgQCrf4i6ApUuXsuOOOzZk7Mxk8eLFLFy4kEGDBnX4OC9tSJJazvLly9l1110NEdtQRLDrrrtu8SyQQUKS1JIMEdteSc8NEpIkddBVV13Fa6+9VrfxBg4cyMsvv9zUGraW90hIklrewAt/UNfxFlw+vui4q666ilNOOYV+/frVtZ6OWr16ddNr2JBBQpKkdrz66quccMIJLFy4kNWrV/Oxj32MRYsWMW7cOHbbbTfuu+8+zj77bB555BFef/11jj/+eC655BKgMtMwadIk7rrrLlauXMltt93GgQceyOLFiznppJN46aWXOPzww6n9Bu4Pf/jDPP/88yxfvpzJkydz5plnAtC/f38+/elP85Of/IRjjjnmTTU0m5c2JElqx49//GP23HNPHn/8cZ544gnOO+889txzT+677751v8D/8R//kUcffZRZs2Zx//33M2vWrHXH77bbbvzyl7/k7LPP5oorrgDgkksu4T3veQ+PPfYYEyZM4Lnnnlu3/7e+9S1mzJjBo48+ytVXX83ixYuBSqAZPnw4Dz/8MBdeeOGbamg2g4QkSe0YMWIE99xzDxdccAE///nP2Wmnnd60z3e+8x0OOeQQRo0axZw5c3jyySfXbfvIRz4CwKGHHsqCBQsAeOCBBzjllFMAGD9+PLvsssu6/a+++moOPvhg3v3ud/P8888zb948AHr27MlHP/rRRp3mVvPShiRJ7TjggAOYMWMGP/zhD7nooot4//vfv972+fPnc8UVV/DII4+wyy67cOqpp6736GSfPn2AShBYtWrVuvXtPRnR1tbGPffcw0MPPUS/fv0YO3bsurH69u1Lz549G3GKdeGMhCRJ7Vi0aBH9+vXjlFNO4bOf/Sy//OUv2XHHHVm6dCkAf/jDH9hhhx3Yaaed+O1vf8uPfvSjzY551FFHcfPNNwPwox/9iN///vcALFmyhF122YV+/frx9NNP84tf/GKjY9TW0AqckZAkqR2zZ8/mb//2b+nRowe9e/fm61//Og899BAf/OAHefvb3859993HqFGjGDZsGPvssw9jxozZ7Jhf/OIXOemkkzjkkEN473vfyzve8Q4APvCBD/CNb3yDgw46iMGDB/Pud797o2OceeaZ69XQbFF7x6g6ZvDgwTl37txml9EltLW1MXbs2GaX0SXYy/qyn/VT0sunnnqKIUOGNKagTqyRH5G9Vnu9j4gZmXlYe/s7I1Fg+arljLhpRFNrmD1pdlPfX5Ik8B4JSZK0FQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkor5+KckqfVNefP3XGzdeEvqO95WWLFiBePHj+fll1/moosu4sQTT1y37bbbbmPKlCk89dRTTJ8+ncGDB6/bdtlll3HDDTfQs2dPrr76ao499lig8mVjkydPZvXq1ZxxxhlceOGFDa3fICFJUhM99thjrFy5kpkzZ75p2/Dhw7n99ts566yz1lv/5JNPMm3aNObMmcOiRYs45phj+NWvfgXApz71Ke6++24GDBjA6NGjmTBhAkOHDm1Y/V7akCRpAwsWLODAAw/kjDPOYPjw4Zx88sncc889jBkzhv3335/p06cDMH36dI444ghGjRrFEUccwdpPPb7yyis5/fTTgcpHbQ8fPpzXXnvtTe/z4osvcsoppzBz5kxGjhzJs88+u972IUOGrDcLsdadd97JxIkT6dOnD4MGDWK//fZj+vTpTJ8+nf3224999tmH7bbbjokTJ3LnnXfWuz3rMUhIktSOZ555hsmTJzNr1iyefvppbrnlFh588EGuuOIKvvSlLwFw4IEH8sADD/DYY49x6aWXcvHFFwNw3nnn8cwzz3DHHXdw2mmn8c1vfpN+/fq96T322GMPrr/+eo488khmzpzJvvvu26HaXnjhBfbee+91ywMGDOCFF17Y6PpG8tKGJEntGDRoECNGVL4OYdiwYRx99NFEBCNGjGDBggVA5Vs7J02axLx584gIVq5cCUCPHj2YOnUqBx10EGeddVaHvtBrS7T3PVkRwZo1a9pd30gGiQJ9cw2z5z9XPkAL3eQjSWpfnz591r3u0aPHuuUePXqwatUqAD7/+c8zbtw47rjjDhYsWLDel5PNmzeP/v37s2jRorrXNmDAAJ5//vl1ywsXLmTPPfcE2Oj6RvHShiRJhZYsWcJee+0FwNSpU9dbP3nyZB544AEWL17Md7/73bq+74QJE5g2bRorVqxg/vz5zJs3j8MPP5zRo0czb9485s+fzxtvvMG0adOYMGFCXd97Qy0zIxERyzKzf83yqcBhwHeByzLzT2u29QJeAEZm5m9q1k8BlmXmFduqbknSNtCiM7mf+9znmDRpEldeeSXve9/71q0///zzOeecczjggAO44YYbGDduHEcddRR77LHHFo1/xx13cO655/LSSy8xfvx4hg8fzs9+9jOGDRvGCSecwNChQ+nVqxfXXnstPXv2BOCaa67h2GOPZfXq1Zx++ukMGzasrue8oWjvOkszbCJI/A3wa+DIzFxQ3fYB4G8z8+gNxpjCNggSg/fZO+d+/A/lA7ToP4hmaGtrW28qUOXsZX3Zz/op6eVTTz3FkCFDGlNQJ7Z06VJ23HHHhr5He72PiBmZeVh7+7f8pY3MXAPcBpxYs3oicOumjouIfSPixxExIyJ+HhEHVtdPjYivR8R9EfE/EfHeiPhWRDwVEVMbdiKSJHVBLXNpA9g+Imo/jeOtwPerr28FrgO+HBF9gA8B529mvOuAT2bmvIh4F/AvwNp5p12qrycAdwFjgDOARyJiZGa++VNBJEnaCjfeeCNf+9rX1ls3ZswYrr322iZVVB8tf2kjM/+6ujwP+D/AEOD0zHzT3SNrL20A3wBeAubWbO6TmUOqsw53Z+bNEbEP8JPM3L96/L8Ct2fm99oZ+0zgTIDdd9vt0O9c/XflJ/v2keXHdjHLli2jf//+m99Rm2Uv68t+1k9JL3faaSf222+/BlXUea1evXrdvRCN8swzz7BkyfqX4MeNG7fRSxutNCOxOdOoXNIYwmYua1C5ZPNKZm7sN/aK6p9ral6vXW63J5l5HZVZDgbvs3eOnfvFDpbdjpO8R2Itr0PXj72sL/tZP6X3SDT6XoDOaFvcI9G3b19GjRrV4f1b/h6JGrcCp1C5JPH9Te2YmX8A5kfExwCi4uDGlyhJUvfSaYJEZj4JvAbcm5mvduCQk4FPRMTjwBzguEbWJ0lSd9QylzZq74+oLk8Fpm6wbpOzCpk5peb1fOAD7exzas3rBcDw9rZJkqTNa5kgIUnSxoy4aURdx5s9aXZdx9saK1asYPz48bz88stcdNFFnHjiHz/t4LbbbmPKlCk89dRTTJ8+fb1vAr3sssu44YYb6NmzJ1dffTXHHnssAD/+8Y+ZPHkyq1ev5owzzuDCCy8EYP78+UycOJHf/e53HHLIIXz7299mu+222+r6DRIFXqcPA5ffUj7AhT/Y6hoWXD5+q8eQJDXfY489xsqVK5k5882fPDB8+HBuv/12zjrrrPXWP/nkk0ybNo05c+awaNEijjnmGH71q18B8KlPfYq7776bAQMGMHr0aCZMmMDQoUO54IILOP/885k4cSKf/OQnueGGGzj77LO3uv5Oc4+EJEnbyoIFCzjwwAM544wzGD58OCeffDL33HMPY8aMYf/992f69OkATJ8+nSOOOIJRo0ZxxBFHMHdu5VMHrrzySk4//XQAZs+ezfDhw3nttdfe9D4vvvgip5xyCjNnzmTkyJE8++yz620fMmTIerMQa915551MnDiRPn36MGjQIPbbbz+mT5/O9OnT2W+//dhnn33YbrvtmDhxInfeeSeZyb333svxxx8PwKRJk/je9970SQdFDBKSJLXjmWeeYfLkycyaNYunn36aW265hQcffJArrriCL33pSwAceOCBPPDAAzz22GNceumlXHzxxQCcd955PPPMM9xxxx2cdtppfPOb36Rfv35veo899tiD66+/niOPPJKZM2ey7777dqi2F154gb333nvd8oABA3jhhRc2un7x4sXsvPPO9OrVa7319eClDUmS2jFo0CBGjKjcmzFs2DCOPvpoIoIRI0awYMECoPItn5MmTWLevHlEBCtXrgQqXzU+depUDjroIM466yzGjBlT19ra+zDJiGDNmjXtrt/Y/vXgjIQkSe3o06fPutc9evRYt9yjRw9WrVoFwOc//3nGjRvHE088wV133cXy5cvXHTNv3jz69+/PokWL6l7bgAEDeP7559ctL1y4kD333HOj63fbbTdeeeWVdXWvXV8PBglJkgotWbKEvfbaC4CpU6eut37y5Mk88MADLF68mO9+97t1fd8JEyYwbdo0VqxYwfz585k3bx6HH344o0ePZt68ecyfP5833niDadOmMWHCBCKCcePGravjpptu4rjj6vPxSl7akCS1vFZ6XLPW5z73OSZNmsSVV17J+973vnXrzz//fM455xwOOOAAbrjhBsaNG8dRRx3FHnvssUXj33HHHZx77rm89NJLjB8/nuHDh/Ozn/2MYcOGccIJJzB06FB69erFtddeu+47OK655hqOPfZYVq9ezemnn86wYcMA+PKXv8zEiRP5+7//e0aNGsUnPvGJuvSgZb60qzN5xz77ZY8Tvrb5HRuoqzz+6fcZ1I+9rC/7WT+l37UxZMiQxhTUiW2L79por/cRsdEv7fLShiRJKualDUmStoEbb7yRr31t/dnsMWPGcO211zapovowSEiStA2cdtppnHbaac0uo+68tCFJaknew7ftlfTcGYkC2/fuydwucrOjJLWivn37snjxYnbddde6fXCSNi0zWbx4MX379t2i4wwSkqSWM2DAABYuXMhLL73U7FJayvLly7f4F/2W6Nu3LwMGDNiiYwwSkqSW07t3bwYNGtTsMlpOW1sbo0aNanYZ6/EeCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSivVqdgGd0fJVyxlx04hml7HVZk+a3ewSJEmdnDMSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiftdGgb69+vo9FZIk4YyEJEnaCts8SETEsg2WT42IayJibEQ8tMG2XhHx24h4ewfGHRkRH6p3vZIkaeNaaUbiAWBARAysWXcM8ERm/qYDx48EDBKSJG1DLRMkMnMNcBtwYs3qicCtG+4bER+LiCci4vGIeCAitgMuBU6MiJkRcWJE7BAR34qIRyLisYg4rnrsqRHxvYi4KyLmR8RfR8Snq/v8IiLeui3OV5KkrqAZN1tuHxEza5bfCny/+vpW4DrgyxHRh8oMw/ntjPEF4NjMfCEids7MNyLiC8BhmfnXABHxJeDezDw9InYGpkfEPdXjhwOjgL7AM8AFmTkqIr4KfBy4qq5nLElSF9WMIPF6Zo5cuxARpwKHAWTmIxHRPyIGA0OAX2Tm79sZ47+AqRHxHeD2jbzP+4EJEfHZ6nJf4B3V1/dl5lJgaUQsAe6qrp8NHNTeYBFxJnAmwO67705bW1tHzlWbsWzZMntZJ/ayvuxn/djL+mnFXrbi45/TqFzSGEI7lzUAMvOTEfEuYDwwMyJGtrNbAB/NzLnrrawct6Jm1Zqa5TVspCeZeR2V2RIGDx6cY8eO7ej5aBPa2tqwl/VhL+vLftaPvayfVuxly9wjUeNW4BTgffzxksd6ImLfzHw4M78AvAzsDSwFdqzZ7SfAuRER1WNGNbRqSZK6oZYLEpn5JPAalfsbXt3Ibl+JiNkR8QSVpz0eB+4Dhq692RL4B6A3MKu63z9sg/IlSepWtvmljczsv8HyVGDqBusO3swYH2ln9e+A0RusO6udY9d7v8wcuKlaJEnSxrXcjIQkSeo8WvFmy5b3+srVDLzwB80uY6stuHx8s0uQJHVyzkhIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkor5XRsFtu/dk7l+T4UkSc5ISJKkcgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKK9Wp2AZ3R8lXLGXHTiGaXUXezJ81udgmSpE7GGQlJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjG/a6NA31zD7PnPNbsMSZKazhkJSZJUbJsGiYjYOSLO2WDdVyJiTvXPT0bEx9s5bmBEPFGzfGtEzIqI8zfYb0pEfLZxZyBJkmpt60sbOwPnAP9Ss+4sYPfMXNGRASLiT4AjMvOdDahPkiRtgW0dJC4H9o2ImcDdwGBgB+DhiLgMGAIsy8wrIuJQ4FvAa8CDNWP8FNijOsa5mfnz9t4oIvYFrgV2r47xV5n5dERMBV4HDgTeCZwGTAL+FHg4M0+t7ylLktR1besgcSEwPDNHrl0REcvWLkfElJp9b6QSFO6PiK/UrJ8A/GftGBtxHfDJzJwXEe+iMgvyvuq2XaqvJwB3AWOAM4BHImJkZs4sPkNJkrqRlnxqIyJ2AnbOzPurq74NfHALju8PHAHcFhFrV/ep2eWuzMyImA38NjNnV4+bAwwE3hQkIuJM4EyA3XfbjbbBl2zROXUKbW3b/C2XLVtGWxPetyuyl/VlP+vHXtZPK/ayJYMEEEBuxfE9gFc2MWux9n6MNTWv1y6325PMvI7KLAeD99k7x8794laU16JOWrLN37KtrY2xY8du8/ftiuxlfdnP+rGX9dOKvdzWj38uBXbc3E6Z+QqwJCLeU1118pa8SWb+AZgfER8DiIqDt7RYSZK0ads0SGTmYuC/IuKJDe57aM9pwLUR8RCVmyO31MnAJyLicWAOcFzBGJIkaRO2+aWNzPzLDZb717yeUvN6BlA7izClun4BMHwjY9cePx/4QDv7nFrzer2xfGJDkqQt4ydbSpKkYq16s2VLe50+DFx+S7PLqLsFzS5AktTpOCMhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSrmd20U2L53T+ZePr7ZZUiS1HTOSEiSpGKbnJGIiE9vantmXlnfciRJUmeyuUsbO1b/HAyMBr5fXf5z4IFGFSVJkjqHTQaJzLwEICJ+ChySmUury1OA2xpenSRJamkdvUfiHcAbNctvAAPrXo0kSepUOvrUxreB6RFxR3X5w8BNjSlJkiR1Fh0KEpn5jxHxI+BIIIHTMvOxhlYmSZJa3pZ8jsRqYA2VILGmMeVIkqTOpEP3SETEZOBmYDdgD+DfIuLcRhYmSZJaX0dnJD4BvCszXwWIiC8DDwH/3KjCJElS6+voUxtB5dLGWqur6yRJUjfW0RmJG4GHN3hq44bGlCRJkjqLjj61cWVE3A+MoTIT4VMbkiRpi57amAn8Zu0xEfGOzHyuIVVJkqROoUNBovqExheB3/LH+yMSOKhxpUmSpFbX0RmJycDgzFzcyGIkSVLn0tGnNp4HljSyEEmS1PlsckYiIj5dffk/QFtE/ABYsXZ7Zl7ZwNokSVKL29yljR2rfz5X/dmu+gOVeyQkSVI3tskgkZmXAETExzLzttptEfGxRhYmSZJaX0fvkbiog+skSVI3srl7JD4IfAjYKyKurtn0FmBVIwuTJEmtb3P3SCwCHgUmADNq1i8Fzm9UUZIkqXPY3D0SjwOPR8Qt1X3fkZlzt0llkiSp5XX0HokPUPmI7B8DRMTIiPh+w6qSJEmdQkeDxBTgcOAVgMycCQxsTEmSJKmz6GiQWJWZfrKlJElaT0e/a+OJiPhLoGdE7A/8DfDfjStLkiR1Bh2dkTgXGEbl47FvofK9G5MbVZQkSeocOhokhlZ/egF9geOARxpVlCRJ6hw6emnjZuCzwBPAmsaVI0mSOpOOBomXMvOuhlYiSZI6nY4GiS9GxPXAz1j/a8Rvb0hVkiSpU+hokDgNOBDozR8vbSRgkJAkqRvraJA4ODNHNLQSSZLU6XQ0SPwiIoZm5pMNraaTWL5qOSNuMlfVw9n9z+bcm87dqjFmT5pdp2okSVuqo0HiPcCkiJhP5R6JADIzD2pYZZIkqeV1NEh8oKFVSJKkTqlDQSIzf93oQiRJUufT0U+2lCRJehODhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUrGOfkS2avTt1dcviqqTtrY2Zn/UXkpSZ+WMhCRJKtaQIBERO0fEORus+0pEzKn++cmI+Hg7xw2MiCdqlm+NiFkRcX4H3nNkRHyoPmcgSZI6olGXNnYGzgH+pWbdWcDumbmiIwNExJ8AR2TmOzv4niOBw4AfbkmhkiSpXKOCxOXAvhExE7gbGAzsADwcEZcBQ4BlmXlFRBwKfAt4DXiwZoyfAntUxzg3M3++dkNEfAz4IrAaWAIcA1wKbB8R7wEuA/4T+GdgRPU8p2TmnRFxKvBhoEqEtgEAABAUSURBVCcwHPgnYDvg/wIrgA9l5u/q3xJJkrqeRgWJC4HhmTly7YqIWLZ2OSKm1Ox7I5WgcH9EfKVm/QTgP2vHqPEF4NjMfCEids7MNyLiC8BhmfnX1ff4EnBvZp4eETsD0yPinurxw4FRQF/gGeCCzBwVEV8FPg5ctfUtkCSp62vqUxsRsROwc2beX131beCDHTj0v4CpEfEd4PaN7PN+YEJEfLa63Bd4R/X1fZm5FFgaEUuAu6rrZwMHbaTWM4EzAXbffXfa2to6UKY2Z9myZfayTuxlfdnP+rGX9dOKvWz2458B5JYelJmfjIh3AeOBmRHR3qxFAB/NzLnrrawcV3ufxpqa5TVspCeZeR1wHcDgwYNz7NixW1q22tHW1oa9rA97WV/2s37sZf20Yi8b9fjnUmDHze2Uma8AS6r3NQCc3JHBI2LfzHw4M78AvAzs3c57/gQ4NyKiesyoLahfkiR1QEOCRGYuBv4rIp7Y4L6H9pwGXBsRDwGvd/AtvhIRs6uPij4APA7cBwyNiJkRcSLwD0BvYFZ1v38oOhlJkrRRDbu0kZl/ucFy/5rXU2pezwAOrtl1SnX9Aio3RbY39kfaWf07YPQG685q59ipwNSa5YEb2yZJkjbNT7aUJEnFmn2zZaf0+srVDLzwB80uo0v4zIhVnLqVvVxw+fg6VSNJ2lLOSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSivldGwW2792TuX6/Q120tbWx4OSxzS5DklTIGQlJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUrFezS6gM1q+ajkjbhrR7DK6hLP7n825N53b7DK6BHtZX63Yz9mTZje7BOlNnJGQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBXzuzYK9M01zJ7/XLPL6BLaBr+x9b2csqQ+xXRybW1tzP6o38VQL/ZT6hhnJCRJUrGWChIR8ScRMS0ino2IJyPihxFxQETMj4jBG+x7VUR8boN1AyPiiW1btSRJ3VfLBImICOAOoC0z983MocDFwNuAacDEmn17AMcD/96MWiVJUkXLBAlgHLAyM7+xdkVmzszMnwO3UhMkgKOABZn5640NFhE9I+IrEfFIRMyKiLOq68dGxP0R8Z2I+FVEXB4RJ0fE9IiYHRH7Nuj8JEnqclopSAwHZrS3ITNnAWsi4uDqqolUwsWmfAJYkpmjgdHAX0XEoOq2g4HJwAjg/wIHZObhwPXAuVt1FpIkdSOd6amNW4GJETEHOA74wmb2fz9wUEQcX13eCdgfeAN4JDN/AxARzwI/re4zm8rMyJtExJnAmQC777YbbYMv2YpT0VrL+uy59b1sa6tLLZ3dsmXLaLMXdWM/68de1k8r9rKVgsQcKvc9bMytVH7h3w/MyswXNzNeAOdm5k/WWxkxFlhRs2pNzfIaNtKTzLwOuA5g8D5759i5X9zM26sj2gZfwlb38iQf/4TK44pjx45tdhldhv2sH3tZP63Yy1a6tHEv0Cci/mrtiogYHRHvBcjMZ4HFwOVs/rIGwE+AsyOid3WsAyJih/qXLUlS99UyQSIzE/gL4M+qj3/OAaYAi2p2uxU4kMrTHZtzPfAk8MvqI6HfpLVmYCRJ6vRa6hdrZi4CTtjE9q8CX93E9gVUbtokM9dQeXz04g12a6v+rD1mbM3r9bZJkqRNa5kZCUmS1Pm01IxEZ/E6fRi4/JZml9ElfGbNKk7d2l5e+IP6FNPJfWbEKk61F3XTiv1ccPn4ZpcgvYkzEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYn7XRoHte/dkrp95XxdtbW0sOHlss8voEuxlfdlPqWOckZAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFTNISJKkYgYJSZJUzCAhSZKKGSQkSVIxg4QkSSpmkJAkScUMEpIkqZhBQpIkFYvMbHYNnc47931nvuULb2l2GV3C2f3P5uvLvt7sMroEe1lf9rN+7GX9dKSXsyfNrvv7RsSMzDysvW3OSEiSpGIGCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSsV7NLqAz6ptrmD3/ufoNOGVJ/cbqZNra2pj90fp/wUx3ZC/ry37Wj72sn1bspTMSkiSpWFOCRET8SURMi4hnI+LJiPhhRBwQEfMjYvAG+14VEZ/r4LgXN6ZiSZLUnm0eJCIigDuAtszcNzOHAhcDbwOmARNr9u0BHA/8eweHN0hIkrQNNWNGYhywMjO/sXZFZs7MzJ8Dt1ITJICjgAWZ+evaASLi7RHxQETMjIgnIuLIiLgc2L667ubqfqdExPTqum9GRM/q+mUR8eWImBER90TE4RHRFhH/ExETGt0ASZK6imYEieHAjPY2ZOYsYE1EHFxdNZFKuNjQXwI/ycyRwMHAzMy8EHg9M0dm5skRMQQ4ERhT3W81cHL1+B2ozIgcCiwF/j/gz4C/AC6tx0lKktQdtOJTG7cCEyNiDnAc8IV29nkE+FZE9Aa+l5kz29nnaOBQ4JHK1RS2B16sbnsD+HH19WxgRWaujIjZwMD2ioqIM4EzAXbfbTfaBl9ScGob0dZWv7E6mWXLltHWjc+/nuxlfdnP+rGX9dOKvWxGkJhD5b6HjbkV+ClwPzArM1/ccIfMfCAijgLGA9+OiK9k5r9usFsAN2XmRe28x8rMzOrrNcCK6rhrIqLdnmTmdcB1AIP32TvHzv3iJk5hC53UvR//HDt2bLPL6BLsZX3Zz/qxl/XTir1sxqWNe4E+EfFXa1dExOiIeC9AZj4LLAYup/3LGkTEO4EXM/P/ATcAh1Q3razOUgD8DDg+IvaoHvPW6nGSJKlOtnmQqM4E/AXwZ9XHP+cAU4BFNbvdChxI5emO9owFZkbEY8BHga9V118HzIqImzPzSeDvgZ9GxCzgbuDtdT4dSZK6tabcI5GZi4ATNrH9q8BXN7H9JuCmdtZfAFxQs/zvtPPoaGb2r3k9ZWPbJEnSpvnJlpIkqVj88Z5DddQ79tkve5zwtc3vqM36zIhV/NPsVnx4qPOxl/VlP+vHXtZPR3q54PLxdX/fiJiRmYe1t80ZCUmSVMwgIUmSihkkJElSMYOEJEkqZpCQJEnFDBKSJKmYQUKSJBUzSEiSpGIGCUmSVMwgIUmSihkkJElSMT/8vMD2vXsytwGfZd4dtbW1seDksc0uo0uwl/VlP+vHXtZPK/bSGQlJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxQwSkiSpmEFCkiQVM0hIkqRiBglJklQsMrPZNXQ6EbEUmNvsOrqI3YCXm11EF2Ev68t+1o+9rJ9m9fKdmbl7ext6betKuoi5mXlYs4voCiLiUXtZH/ayvuxn/djL+mnFXnppQ5IkFTNISJKkYgaJMtc1u4AuxF7Wj72sL/tZP/ayflqul95sKUmSijkjIUmSinX7IBERH4iIuRHxTERc2M72PhHx79XtD0fEwJptF1XXz42IYzs6ZldV715GxN4RcV9EPBURcyJi8rY7m+ZrxN/N6raeEfFYRPxn48+iNTTo3/nOEfHdiHi6+nf0T7fN2TRfg/p5fvXf+RMRcWtE9N02Z9Ncpb2MiF2r/31cFhHXbHDMoRExu3rM1RERDT2JzOy2P0BP4FlgH2A74HFg6Ab7nAN8o/p6IvDv1ddDq/v3AQZVx+nZkTG74k+Devl24JDqPjsCv+oOvWxUP2uO+zRwC/CfzT7PztxL4CbgjOrr7YCdm32unbWfwF7AfGD76n7fAU5t9rm2eC93AN4DfBK4ZoNjpgN/CgTwI+CDjTyP7j4jcTjwTGb+T2a+AUwDjttgn+Oo/AcD4LvA0dV0dxwwLTNXZOZ84JnqeB0Zsyuqey8z8zeZ+UuAzFwKPEXlPzjdQSP+bhIRA4DxwPXb4BxaRd17GRFvAY4CbgDIzDcy85VtcC6toCF/N6l8rtH2EdEL6AcsavB5tILiXmbmq5n5ILC8dueIeDvwlsx8KCup4l+BDzfyJLp7kNgLeL5meSFv/kW1bp/MXAUsAXbdxLEdGbMrakQv16lO540CHq5jza2sUf28CvgcsKb+JbesRvRyH+Al4MbqZaLrI2KHxpTfcurez8x8AbgCeA74DbAkM3/akOpby9b0clNjLtzMmHXV3YNEe9eNNnyMZWP7bOn6rq4RvawcFNEf+A/gvMz8Q3GFnUvd+xkR/wd4MTNnbG1xnUwj/m72Ag4Bvp6Zo4BXge5yP1Qj/m7uQuX/eQ8C9gR2iIhTtqrKzmFrerk1Y9ZVdw8SC4G9a5YH8ObptHX7VKfcdgJ+t4ljOzJmV9SIXhIRvamEiJsz8/aGVN6aGtHPMcCEiFhAZQr1fRHxb40ovsU06t/5wsxcO0P2XSrBojtoRD+PAeZn5kuZuRK4HTiiIdW3lq3p5abGHLCZMeuquweJR4D9I2JQRGxH5UaW72+wz/eBSdXXxwP3Vq87fR+YWL2jdhCwP5UbXDoyZldU915Wr6neADyVmVduk7NoHXXvZ2ZelJkDMnNgdbx7M7M7/L++RvTyf4HnI2Jw9ZijgScbfSItohH/3XwOeHdE9Kv+uz+ayj1RXd3W9LJdmfkbYGlEvLvay48Dd9a/9PXftFv/AB+i8jTAs8DfVdddCkyovu4L3EblpqDpwD41x/5d9bi51NwV296Y3eGn3r2kckdyArOAmdWfDzX7PDtrPzcYeyzd5KmNRvUSGAk8Wv37+T1gl2afZyfv5yXA08ATwLeBPs0+z07QywVUZieWUZmJGFpdf1i1j88C11D98MlG/fjJlpIkqVh3v7QhSZK2gkFCkiQVM0hIkqRiBglJklTMICFJkooZJCRJUjGDhCRJKmaQkCRJxf5/Hk4MMAyqCGUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>standart</th>\n",
       "      <th>max_f 100</th>\n",
       "      <th>max_f 1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>CV stem</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CV lem</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tfidf stem</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tfidf lem</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HV stem</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HV lem</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            standart  max_f 100  max_f 1000\n",
       "method                                     \n",
       "CV stem       0.0102     0.0005      0.0102\n",
       "CV lem        0.0069     0.0024      0.0069\n",
       "tfidf stem    0.0024     0.0000      0.0024\n",
       "tfidf lem     0.0013     0.0013      0.0013\n",
       "HV stem       0.0012     0.0000      0.0012\n",
       "HV lem        0.0008     0.0004      0.0008"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for feature in (100, 1000):\n",
    "    \n",
    "    for key_vac, val_vec in {\n",
    "        'CV': text.CountVectorizer(max_df=0.9, max_features=feature, stop_words='english'), \n",
    "        'tfidf': text.TfidfVectorizer(max_df=0.9, max_features=feature, stop_words='english'), \n",
    "        'HV': text.HashingVectorizer(n_features=feature, stop_words='english')\n",
    "    }.items():\n",
    "\n",
    "        for key_tweet, val_tweet in {\n",
    "            'stem': X_train_stem, \n",
    "            'lem': X_train_lem\n",
    "        }.items():\n",
    "\n",
    "            X_train = val_vec.fit_transform(val_tweet)\n",
    "            X_test = val_vec.transform(X_corpus)\n",
    "\n",
    "            classifier = LogisticRegression()\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "            results.loc[f'{key_vac} {key_tweet}', f'max_f {feature}'] = accuracy_score(y_corpus, y_test_pred)\n",
    "\n",
    "results.plot(kind='barh', grid=True, title='Accuracy score', figsize=(8, 8))\n",
    "plt.show()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf - изменений нет. CV stem - увеличение фичей, увеличение скора. HV - аналогично. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - не исследовал. Только Logistic regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
